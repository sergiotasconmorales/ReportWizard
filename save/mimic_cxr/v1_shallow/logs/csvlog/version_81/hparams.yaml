accelerator: gpu
accumulate_grad_batches: 1
annotation: ./data/mimic_cxr/annotation.json
base_dir: /storage/workspaces/artorg_aimi/ws_00000/sergio/radrep/mimic-cxr-jpg-google/files
batch_size: 4
beam_size: 3
ckpt_file: null
dataset: mimic_cxr
delta_file: null
devices: 4
diversity_penalty: 0
do_sample: false
end_sym: </s>
every_n_train_steps: 0
freeze_vm: true
global_only: false
gradient_clip_val: 1
learning_rate: 0.0001
length_penalty: 2.0
limit_test_batches: 1.0
limit_train_batches: 1.0
limit_val_batches: 0.5
llama_model: meta-llama/Llama-2-7b-chat-hf
llm_alpha: 16
llm_r: 16
llm_use_lora: false
lora_dropout: 0.1
low_resource: true
max_epochs: 5
max_length: 100
max_new_tokens: 120
min_new_tokens: 80
no_repeat_ngram_size: 2
num_beam_groups: 1
num_nodes: 1
num_sanity_val_steps: 2
num_workers: 4
precision: '16'
prefetch_factor: 4
repetition_penalty: 2.0
savedmodel_path: ./save/mimic_cxr/v1_shallow
scorer_types:
- Bleu_4
- CIDEr
strategy: ddp
temperature: 0
test: false
test_batch_size: 16
val_batch_size: 4
val_check_interval: 0.5
validate: false
vis_alpha: 16
vis_r: 16
vis_use_lora: false
vision_model: microsoft/swin-base-patch4-window7-224
weights:
- 0.5
- 0.5
